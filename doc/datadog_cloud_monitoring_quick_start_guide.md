# Datadog Cloud Monitoring Quick Start Guide

## Chapter 1: Introduction to Monitoring

### Why monitoring?

The primary focus of monitoring a software system is to check its health.
By keeping track of the health of a software system, it is possible to determine whether the system is available or its health is deteriorating.

### Proactive monitoring

Proactive monitoring refers to rolling out monitoring solutions for a software system to report on issues with the components of the software system, and the infrastructure the system runs on.
Such reporting can help with averting an impending issue by taking mitigating steps manually or automatically.
The latter method is usually called self-healing, a highly desirable end state of monitoring, but hard to implement.

The key aspects of a proactive monitoring strategy are as follows.

#### Implementing a comprehensive monitoring solution

#### Setting up alerts to warn of impending issues

The monitoring solution must be designed to warn of impending issues with the software system.
This is easy with infrastructure components as it is easy to track metrics such as memory usage, CPU utilization, and disk space, and alert on any usage over the limits.

However, such a requirement would be tricky at the application level.
Sometimes applications can fail on perfectly configured infrastructure.
To mitigate that, software applications should provide insights into what is going under the hood.
In monitoring jargon, it is called observability these days and we will see later in the book how that can be implemented in Datadog.

#### Having a feedback loop

### Monitoring use cases

#### All in the cloud

There could be two different cases in an all-in-the-cloud scenario of monitoring.
In both cases, the infrastructure that runs the software system will be in the cloud, typically on a public cloud platform such as AWS.
The entire monitoring system can be deployed on the same infrastructure or a cloud monitoring service such as Datadog can be used, in which case, only its agent will be running alongside the application system.

In the first case of an all-in-the-cloud scenario, you need to set up monitoring or leverage monitoring services provided by a public cloud provider such as CloudWatch on AWS or use a combination of both.

In the second case of an all-in-the-cloud scenario, a third-party SaaS monitoring service such as Datadog is used.
The main attractions of using SaaS monitoring services are their rich set of features, the high availability of such services, and the minimal overhead of rolling out and maintaining monitoring.

### Monitoring terminology and processes

Now, let's look at the most commonly used monitoring terminology in both literature and tools.
The difference between some of these terms is subtle and you may have to pay close attention to understand them.

#### Host

A host used to mean a physical server during the data center era.
In the monitoring world, it usually refers to a device with an IP address.
That covers a wide variety of equipment and resources - bare-metal machines, network gear, IoT devices, virtual machines, and even containers.

Some of the first-generation monitoring tools, such as Nagios and Zenoss are built around the concept of a host, meaning everything that can be done on those platforms must be tied to a host.
Such restrictions are relaxed in new-generation monitoring tools such as Datadog.

#### Agent

An agent is a service that runs alongside the application software system to help with monitoring.
It runs various tasks for the monitoring tools and reports information back to the monitoring backend.

The agents are installed on the hosts where the application system runs.
It could be a simple process running directly on the operating system or a microservice.
Datadog supports both options and when the application software is deployed as microservices, the agent is also deployed as a microservice.

#### Metrics

Metrics in monitoring refers to a time-bound measurement of some information that would provide insight into the workings of the system being monitored.
These are some familiar examples:
* Disk space available on the root partition of a machine
* Free memory on a machine
* Days left until the expiration of an SSL certificate

The important thing to note here is that a metric is time-bound and its value will change.
For that reason, the total disk space on the root partition is not considered a metric.

A metric is measured periodically to generate time-series data.
We will see that this time-series data can be used in various ways - to plot charts on dashboards, to analyze trends, and to set up monitors.

A wide variety of metrics, especially those related to infrastructure, are generated by monitoring tools.
There are options to generate your own custom metrics too:
* Monitoring tools provide options to run scripts to generate metrics.
* Applications can publish metrics to the monitoring tool.
* Either the monitoring tool or others might provide plugins to generate metrics specific to third-party tools used by the software system.
For example, Datadog provides such integrations for most of the popular tools, such as NGINX.
So, if you are using NGINX in your application stack by enabling the integration, you can get NGINX-specific metrics.

#### Up/down status

#### Check

A check is used by the monitoring system to collect the value of metrics.
When it is done periodically, time-series data for that metric is generated.

While time-series data for standard infrastructure-level metrics is available out of the box in monitoring systems, custom checks could be implemented to generate custom metrics that would involve some scripting.

A check can be active or passive.
An active check is initiated by the monitoring backend to collect metrics values and up/down status info, with or without the help of its agents.
This is also called the pull method of data collection.

A passive check reports such data to the monitoring backend, typically with its own agents or some custom script.
This is also called the push method of data collection.

The active/passive or pull/push model of data collection is standard across all monitoring systems.
The method would depend on the type of metrics a monitoring system collects.
You will see in later chapters that Datadog supports both methods.

#### Threshold

A threshold is a fixed value in the range of values possible for a metric.
For example, on a root partition with a total disk space of 8 GB, the available disk space could be anything from 0 GB to 8 GB.
A threshold in this specific case could be 1 GB, which could be set as an indication of low storage on the root partition.

There could be multiple thresholds defined too.
In this specific example, 1 GB could be a warning threshold and 500 MB could be a critical or high-severity threshold.

#### Monitor

A monitor looks at the time-series data generated for a metric and alerts the alert recipients if the values cross the thresholds.
A monitor can also be set up for the up/down status, in which case it alerts the alert recipients if the related resource is down.

#### Alert

An alert is produced by a monitor when the associated check determines that the metric value crosses a threshold set on the monitor.
A monitor could be configured to notify the alert recipients of the alerts.

#### Alert recipient

The alert recipient is a user in the organization who signs up to receive alerts sent out by a monitor.
An alert could be received by the recipient through one or more communication channels such, as E-Mail, SMS, and Slack.

#### Severity level

Alerts are classified by the seriousness of the issue that they unearth about the software system and that is set by the appropriate severity level.
The response to an alert is tied to the severity level of the alert.

A sample set of severity levels could consist of the levels Informational, Warning, and Critical.
For example, with our example of available disk space on the root partition, at 30% of available disk space, the monitor could be configured to alert as a warning and at 20% it could alert as critical.

As you can see, setting up severity levels for an increasing level of seriousness would provide the opportunity to catch issues and take mitigative actions ahead of time, which is the main objective of proactive monitoring.
Note that this is possible in situations where a system component degenerates over a period of time.

A monitor that tracks an up/down status will not be able to provide any warning, and so a mitigative action would be to bring up the related service at the earliest.
However, in a real-life scenario, there must be multiple monitors so at least one of them would be able to catch an underlying issue ahead of time.
For example, having no disk space on the root partition can stop some services, and monitoring the available space on the root partition would help prevent those services from going down.

#### Notification

A message sent out as part of an alert specific to a communication platform such as email is called a notification.
There is a subtle difference between an alert and a notification, but at times they are considered the same.
An alert is a state within the monitoring system that can trigger multiple actions such as sending out notifications and updating monitoring dashboards with that status.

Traditionally, email distribution groups used to be the main communication method used to send out notifications.
Currently, there are much more sophisticated options, such as chat and texting, available out of the box on most monitoring platforms.
Also, escalation tools such as PagerDuty could be integrated with modern monitoring tools such as Datadog to route notifications based on severity.

#### Downtime

The downtime of a monitor is a time window during which alerting is disabled on that monitor.
Usually, this is done for a temporary period while some change to the underlying infrastructure or software component is rolled out, during which time monitoring on that component is irrelevant.
For example, a monitor that tracks the available space on a disk drive will be impacted while the maintenance task to increase the storage size is ongoing.

Monitoring platforms such as Datadog support this feature.
The practical use of this feature is to avoid receiving notifications from the impacted monitors.
By integrating a CI/CD pipeline with the monitoring application, the downtimes could be scheduled automatically as a prerequisite for deployments.

#### Event

An event published by a monitoring system usually provides details of a change that happened in the software system.
Some examples are the following:
* The restart of a process
* The deployment or shutting down of a microservice due to a change in user traffic
* The addition of a new host to the infrastructure
* A user logging into a sensitive resource

Note that none of these events demand immediate action but are informational.
That's how an event differs from an alert.
A critical alert is actionable but there is no severity level attached to an event and so it is not actionable.
Events are recorded in the monitoring system and they are valuable information when triaging an issue.

#### Incident

When a product feature is not available to users it is called an incident.
An incident occurs when some outage happens in the infrastructure, this being hardware or software, but not always.
It could also happen due to external network or internet-related access issues, though such issues are uncommon.

The process of handling incidents and mitigating them is an area by itself and not generally considered part of core monitoring.
However, monitoring and incident management are intertwined for these reasons:
* Not having comprehensive monitoring would always cause incidents because, without that, there is no opportunity to mitigate an issue before it causes an outage.
* And of course, action items from a Root Cause Analysis (RCA) of an incident would have tasks to implement more monitors, a typical reactive strategy (or the lack thereof) that must be avoided.

#### On call

The critical alerts sent out by monitors are responded to by an on-call team.
Though the actual requirements can vary based on the **Service-Level Agreement (SLA)** requirements of the application being monitored, on-call teams are usually available 24x7.

In a mature service engineering organization, three levels of support would be available, where an issue is escalated from L1 to L3:
* The L1 support team consists of product support staff who are knowledgeable about the applications and can respond to issues using runbooks.
* The L2 support team consists of Site Reliability Engineers (SREs) who might also rely on runbooks, but they are also capable of triaging and fixing the infrastructure and software components.
* The L3 support team would consist of the DevOps and software engineers who designed and built the infrastructure and software system in production.
Usually, this team gets involved only to triage issues that are not known.

#### Runbook

A runbook provides steps to respond to an alert notification for on-call support personnel.
The steps might not always provide a resolution to the reported issue and it could be as simple as escalating the issue to an engineering point of contact to investigate the issue.

### Types of monitoring

There are different types of monitoring.
All types of monitoring that are relevant to a software system must be implemented to make it a comprehensive solution.
Another aspect to consider is the business need of rolling out a certain type of monitoring.
For example, if customers of a software service insist on securing the application they subscribe to, the software provider has to roll out security monitoring.

#### Infrastructure monitoring

The infrastructure that runs the application system is made up of multiple components: servers, storage devices, a load balancer, and so on.
Checking the health of these devices is the most basic requirement of monitoring.
The popular monitoring platforms support this feature out of the box.
Very little customization is required except for setting up the right thresholds on those metrics for alerting.

#### Platform monitoring

#### Application monitoring

#### Business monitoring

### Overview of monitoring tools

## Chapter 2: Deploying the Datadog agent

In the previous chapter, we learned that the cornerstone of a monitoring tool is the group of metrics that helps to check the health of the production system.
The primary tasks of the monitoring tool are to collect metric values periodically as time series data and to alert on issues based on the thresholds set for each metric.

The common method used by monitoring tools to collect such data is to run an agent process close to where the software application runs, be it on a bare-metal server, virtual machine, or container.
This would enable the monitoring agent to collect metric values directly by querying the software application and the infrastructure where it runs.

Datadog collects such data in various ways and like other monitoring tools, it also provides an agent.
The agent gathers monitoring data from the local environment and uploads that to the Datadog SaaS backend in the cloud.
In this chapter, we will learn how the Datadog Agent is configured to run in production environments.

### Installing the Datadog Agent

The Datadog Agent can be configured to run in multiple ways for it to monitor the infrastructure and the processes, including microservices in the environment where it runs.
It can run at the host level and as a microservice and the actual configuration would usually depend on how the application software is deployed.

#### Runtime configurations

There are multiple ways you can deploy the Datadog Agent in runtime environments to collect events and data, and such configurations depend largely on how the applications are deployed.
For example, if all the applications run directly on the host, then the Datadog Agent is run directly on the host as well.
Let's look at the common runtime configurations.

The Datadog Agent can be configured to run in three different ways locally, as illustrated in the diagrams shown as follows.
In all the cases, the agent also collects data on the infrastructure health in addition to collecting application-specific metrics:

In all these three configurations and their variations, the Datadog Agent should be able to connect to the Datadog SaaS backend through the company network and internet, to upload the metrics collected locally.
Therefore, configuring the company network firewall to enable the traffic going out from the Datadog Agent is a prerequisite for it to be operational.
While this network access is allowed by default in most environments, in some restrictive situations, configuring the network suitably is a requirement for rolling out Datadog.

In general, if the application software is deployed as microservices, it is better to also deploy the Datadog Agent as a microservice.
Likewise, in a non-microservice environment, the Datadog Agent is run directly on the hosts.
Maintenance tasks such as version upgrades are very easy if the agent is deployed as a microservice, which is the preferred method in a compatible environment.

#### Steps for installing the agent

Datadog supports a wide variety of client platforms where the agent can be run, such as Windows, Kubernetes, Docker, and all the popular Linux distributions, such as Ubuntu, CentOS, and Red Hat Enterprise Linux.
As an example, we will look at how the agent is installed on an Ubuntu host.
On other operating systems, the steps are similar with changes specific to platform differences accounted for.

Before an agent can be installed, you should sign up for a Datadog account.
Datadog allows you to try out most of its features for free for a 2-week trial period.
Once you have access to an account, you will get access to an API key for that account.
When an agent is installed, the **API key** has to be specified and that's how the Datadog SaaS backend correlates the agent traffic to a customer account.

Once the agent is installed successfully, it will try to connect to the Datadog backend.
The corresponding host will be listed on the dashboard under **Infrastructure | Host Map** and **Infrastructure List** if the agent is able to connect to the backend.
This is a method to quickly verify whether an agent is operational at any time.

On Linux platforms, the logs related to the installation process are available in the **dd_agent.log** log file, which can be found in the current directory where the install script is run.
If the installation process fails, it will provide pointers on what has gone wrong.
The agent log files are available under the **/var/log/datadog** directory.

As mentioned earlier, the steps for installing the Datadog Agent on a specific operating system can be obtained by navigating to the **Integrations | Agent** window.
The supported operating systems and platforms are listed on the left pane, and by clicking on the required one, you can get the steps, as shown in *Figure 2.6* for Ubuntu.

### Agent components

The Datadog Agent is a service that is composed of multiple component processes doing specific tasks.
Let's look at those in detail to understand the workings of the Datadog Agent.

On Ubuntu systems, the Datadog Agent service is named **datadog-agent**.
The runtime status of this service can be checked and maintained using the system command service like any other service.

The **/etc/datadog-agent** directory has all the configuration files related to the Datadog Agent running on that machine.
The YAML **/etc/datadog-agent/datadog.yaml** file is the main configuration file.
If any change is made to this file, the Datadog service needs to be restarted for those changes to take effect.

The **/etc/datadog-agent/conf.d/** directory contains configuration files related to the integrations that are run on that host.
We will see the configuration requirements for integrations and how they are installed in *Chapter 9, Integrating with Platform Components*, which is dedicated to discussing integrating Datadog with cloud platform applications.

There are three main components in the Datadog Agent service:
* **Collector**: As the name suggests, the Collector collects the system metrics every 15 seconds. The collection frequency can be different for other types of metric types and the Datadog documentation provides that information.
* **Forwarder**: The metrics collected locally are sent over HTTPS to the Datadog backend by the Forwarder. To optimize the communication, the metrics collected are buffered in memory prior to shipping them to the backend.
* D**ogStatsD**: StatsD is a general-purpose metrics collection and aggregation daemon that runs on port **8125** by default.
StatsD is a popular interface offered by monitoring tools for integrating with external systems.
DogStatsD is an implementation of StatsD by Datadog and it is available as a component of the Datadog Agent.
We will see later in this book how StatsD can be used to implement lightweight but very effective integrations.

Besides these three components, there are optional processes that can be started by specifying them in the **datadog.yaml** file:
* **APM agent**: This process is needed to support the APM feature and it should be run if the APM feature is used.
* **Process agent**: To collect details on the live processes running on the host, this component of the Datadog Agent process needs to be enabled.
* **Agent UI**: The Datadog Agent also provides a UI component that runs directly on the host where the Datadog Agent is running.
This is not a popular option; the information about a host is usually looked up on the main dashboard, which provides complete insight into your infrastructure and the applications running on it.
However, it could be used for ad hoc purposes, for example, troubleshooting on consumer platforms such as macOS and Windows.

### Agent as a container

As mentioned earlier, the Datadog Agent can be installed as a container on a Docker host.
Though the actual options might differ, the following is a sample command that explains how the Datadog Agent is started up as a container:

### Advanced agent configuration

### Best practices

As you have seen, there are multiple ways to install and configure the Datadog Agent, and, for someone new to Datadog, it could be daunting to determine how the agent can be rolled out and fine-tuned efficiently to meet the monitoring requirements.
However, there are a few things that are obvious as best practices, and let's summarize those here:
* If the agent is installed on the host, plan to include it in the machine image used to spin up or boot the host.
* Set up Ansible playbooks or similar tools to make ad hoc changes to the Datadog Agent on the host.
This is not recommended for some complex infrastructure environments, especially where bare-metal servers are used, so some in-place change might be needed.
* When containers are to be monitored, plan to deploy the agent also as a container.
* Plan to collect tags from underlying infrastructure components such as Docker and Kubernetes by suitably configuring the agent.

## Chapter 3: The Datadog Dashboard

In the previous chapter, we looked at the various ways the Datadog agents running on your infrastructure upload monitoring metrics to Datadog's SaaS backend.
The Datadog dashboard provides an excellent view of that data, and it can be accessed using popular web browsers.
The administrators use the dashboard to perform a variety of tasks - managing user accounts and their privileges, creating operational dashboards, enabling integrations, and creating monitors are typical examples.
The dashboard provides a chat window for the users to contact customer support as well.

While the dashboard has a large number of features, some of them are more important than others, and we will study them in detail in this chapter.
These are the important dashboard features:
* Infrastructure List
* Events
* Metrics Explorer
* Dashboards
* Integrations
* Monitors
* Advanced features

### Infrastructure List

Once an agent is up and running on a host, the agent starts reporting into the Datadog backend in the cloud.
The host will get added to the infrastructure lists once communication between the agent running on it and the backend is successfully established.

The infrastructure lists are available underneath the **Infrastructure** main menu.
The most important ones are **Host Map** and **Infrastructure List**:

Each block in a **Host Map** menu represents a host where an agent is running.
In the following screenshot, the green color indicates that the agent is active and able to communicate with the backend.
The orange color indicates some trouble with communication; however, it also indicates that, at some point in the past, the related agent was able to connect to the backend:

By clicking on a specific block, you can view monitoring details on the related host such as the hostname, the tags defined at the host level, various system-level information, and the list of services being monitored, as shown in the following screenshot:

This host-level drill-down feature to look up various system information avoids the need to log in to a host to check on such details.

As indicated in the following screenshot, there are links available on this interface to take you to various host-specific dashboards.
For example, the **Dashboar**d link will open a dashboard where you can view various infrastructure metrics such as CPU usage, load averages, and disk usage.

**Infrastructure List** offers a tabular view of the hosts, as shown in the following screenshot:

This interface is another view of the host list and not very different from Host Map in terms of the monitoring information it provides on a specific host.

You can view more options, such as Containers and Processes, underneath the Infrastructure menu.
As the names indicate, the related runtime resources can be listed and searched for in the respective dashboards.

### Events

Most monitoring tools only focus on collecting and reporting time-series metrics data.
In comparison, Datadog reports on events too, and that is one of its attractive features.
These are system-level events such as the restarting of a Datadog agent and the deployment of a new container:

The **Events** dashboard can be accessed using the **Events** main menu option on the Datadog UI.
You can directly add an update to the events stream using this dashboard and also comment on an already posted event.
This social media-inspired feature is useful in situations where you need to communicate or add clarity about some system maintenance-generated events to remote teams.

The events listed on the dashboard can be filtered using various options, including **search**.
Additionally, there is an option to aggregate related events, which is useful in adding brevity to the events listing.

**Events** is one of the main menu options, and using it, the dashboard can be accessed.

### Metrics Explorer

The metrics collected by Datadog can be viewed and searched for from the **Metrics Explorer** dashboard, which can be accessed from the main **Metrics** menu:

Soon after the agent running on a host is connected to the Datadog backend, you can begin looking at a variety of metrics that monitor the infrastructure.
This feature is out of the box, and there is no special configuration required:

The infrastructure metric names are prefixed with **system**, and a full list of the metrics that are available out of the box can be found in the Datadog documentation.

In the **Metrics Explorer** dashboard, the metrics name is specified in the **Graph** field.
There is an automatic search feature that pulls possible metric names based on what text string you type in.
Multiple metrics can be specified in this field.

In the **Over** field, various tags can be specified to narrow down the scope when searching for a metric.
We will learn more about tags in *Chapter 5, Custom Metrics and Tags*.
Out of the box, very few tags are available, such as **host**, which points to the hostname.
With very little effort, for instance, by adding a tags entry in the Datadog agent configuration file, tags can be added to the metric.

As a Datadog user, you will use **Metrics Explorer** on a regular basis for the following reasons:
* Looking up the metrics' time-series data pertaining to a specific infrastructure resource or application.
* In addition to looking up the metrics data, creating graphs for the related data to add custom dashboards.
* Triaging issues pertain to publishing custom metrics and tags.
If such customizations work, it will be easy to query the custom metrics and tags using this dashboard and verify their availability.

### Dashboards

A Datadog dashboard is a visual tool that can be used to track and analyze metrics.
It offers a rich set of features that can be used to build custom dashboards, in addition to the dashboards available out of the box.

The **Dashboards** main menu option has two options, **Dashboard List** and **New Dashboard**:

The **Dashboard List** menu will list all the dashboards grouped by various categories.

Two types of custom dashboards can be created in Datadog, **Timeboards** and **Screenboards**:

The graphs in a **Timeboard** dashboard will share the same time frame, so it will be useful for comparing multiple metrics side by side.
This feature makes a **Timeboard** dashboard useful for triaging issues.
The **Screenboard** dashboard doesn't have a time constraint, and disparate objects can be assembled on it.
In that sense, a **Screenboard** type dashboard is more suitable for building a traditional monitoring dashboard.

## Chapter 5: Metrics, Events and Tags

In this chapter, we will explore in detail how metrics and tags, two important constructs that Datadog heavily depends on, are implemented.
Metrics are the basic entities that are used for reporting monitoring information in Datadog.
Datadog also uses tags to organize metrics and other types of monitoring information such as events and alerts.
It is important to discuss metrics and tags together as appropriate tagging of metrics is very important to make sense out of the large volume of metrics time series data that will be available in a typical Datadog account.

While metrics, a central concept in any monitoring system, help to measure the health of a system continuously, an event captures an incident that occurs in a system.
The crashing of a process, the restarting of a service, and the reallocation of a container are examples of system events.
A metric is measured at a specific time interval and there is a numeric value associated with it, but an event is not periodic in nature and provides only a status.
Tagging can be used to organize, group, and search just for events that are used with metrics.

### Understanding metrics in Datadog

The health of a software system and the infrastructure it is running on are defined by a set of metrics and their threshold values.
For example, on the infrastructure side, if the CPU usage on a machine is under 70%, it might be considered healthy for a specific use case.
When all such metrics that are used for monitoring an environment report values in the normal range, the entire environment can be considered healthy.
By setting relevant thresholds for these metrics on monitors, issues can be reported as alerts.
Datadog provides features to define metrics-based monitors and alerts.

#### Metric data

A metric data value is a measure of a metric at a point in time. For example, the **system.memory.free** metric tracks the free memory available on a host.
Datadog measures that metric and reports the value periodically, and that value is the metric data value.
A series of such measurements will generate a time series data stream, as seen plotted in the example in *Figure 5.1*.

#### Flush time interval

The Datadog processes values received for a metric during this time window and processes them based on the metric type.

#### Metric type

The main metric types are count, rate, and gauge, and they differ in terms of how metric data points are processed to publish as metric values:
* **Count**: The metric data received during the flush time interval is added up to be returned as a metric value.
* **Rate**: The total of metric data received during the flush time interval is divided by the number of seconds in the interval to be returned as a metric value.
* **Gauge**: The latest value received during the flush time interval is returned as the metric value.

#### Metric unit

A metric type is a group of similar measurement units.
For example, the bytes group is a type of storage and memory measurement and it consists of units such as bit, byte, kibibyte, mebibyte, gibibyte, and so on.
The time group consists of units from a nanosecond to a week.
A full list of metric units is available in the Datadog documentation (https://docs.datadoghq.com/developers/metrics/units/) for reference.

#### Query

A query returns values from a time series metric dataset, for the given filter conditions and a time window.
For example, the time series data related to the **system.memory.free** metric will be reported for all the machines where the Datadog Agent runs.

If you want to monitor the metric only for a specific host during the last one hour, the Datadog-defined **host** tag can be used to narrow down the machine to a specific host, and you can also specify a time range.
The ways to specify the query parameters would depend on the interface that you would use to run the query.
For example, you have seen how a query works in the **Metrics Explorer** window on the Datadog dashboard where those parameters are specified visually.

Datadog identifies the following parts in a query:
* **Scope**: By specifying the appropriate tags, the results of a query can be narrowed down. In the last example, the **host** tag set the scope to a single host.
* **Grouping**: A metric such as **system.disk.free** would have more than one time series dataset for the same host, as one would be generated for each disk, and typically there would be multiple disks on a host. Assume that you are interested in monitoring the total disk space available on all the web hosts. If those hosts are tagged as web hosts using a custom tag such as **host_type** having the value **web**, then it could be used to set the scope and the **host** tag could be used for grouping.
* **Time aggregation and rollup**: To display the data returned by a query, the data points are aggregated to accommodate them. Datadog returns about 150 data points for a time window. If the time window is large, the data points are aggregated into time buckets and that process is called rollup. The metrics that are available in Datadog out of the box are generated by integrations that are activated as part of installing the Datadog Agent. The main integrations in this category that provide metrics for infrastructure monitoring are the following:
A. System: Generates CPU-, load-, memory-, swap-, IO-, and uptime-related metrics, identified by the patterns such as system.cpu.*, system.load.*, and system.mem.*

B. Disk: Generates disk storage-related metrics, identified by system.disk.*

C. Directory: Generates directory- and files-related metrics, identified by the system.disk.directory.* pattern

D. Process: Generates process-specific compute metrics, identified by the system.processes.* pattern

### Tagging Datadog resources

In the previous sections of this chapter, we have touched upon the concept of tags as a means of grouping and filtering metrics.
While the use of tags is similar to that of keywords, labels, or hashtags on other systems, it is important to learn how tags are applied to metrics.
Tags can be applied to other Datadog resources such as events and monitors as well.
However, we will focus on how they are applied and used while working with metrics in the following section.

#### Defining tags

#### Tagging methods

Datadog offers multiple ways to tag metrics.
However, the following two methods are the most common:
* From the Datadog Agent configuration file
* From the configuration files of integrations

Both of these methods are done at the Datadog Agent level.
The tagging can also be done from the Datadog UI, using the Datadog API, and as part of DogStatsD integration.

In the Datadog Agent config file, **datadog.yml**, the config item tags can be used to add tags as follows:
```yaml
tags: ["KEY1:VALUE1", "KEY2:VALUE2"]
```
Here's an alternative way:
```yaml
tags:
  - "KEY1:VALUE1"
  - "KEY2:VALUE2"
```
The latter syntax is preferred in configuration files for better readability.

#### Customizing host tag

We have come across the use of the system-defined **host** tag already, which is very useful in filtering the metrics data.
It is set by default based on the host name of the machine where the Datadog Agent runs, but it can be customized by setting the **hostname** config item in **datadog.yml**.

## Chapter 7: Monitors an Alerts

In the last chapter, we learned how infrastructure is monitored using Datadog.
The modern, cloud-based infrastructure is far more complex and virtual than the data center-based, bare-metal compute, storage, and network infrastructure.
Datadog is designed to work with cloud-centric infrastructure, and it meets most of the infrastructure monitoring needs out of the box, be it a bare-metal or public cloud-based infrastructure.

A core requirement of any monitoring application is to notify you about an ongoing issue.
Ideally, before that issue results in a service outage.
In previous chapters, we discussed metrics and how they are generated, viewed, and charted on dashboards.
An important use of metrics is to predict an upcoming issue.
For example, by tracking the **system.disk.free** metric on a storage device, it is easy to notify when it reaches a certain point.
By combining the **system.disk.total** metric to that equation, it's also possible to track the available storage as a percentage.

A monitor typically tracks a time-series value of a metric, and it sends out a notification when the metric value is abnormal with reference to a threshold during a specified time window.
The notification that it sends out is called a warning or alert notification.
Such notifications are commonly referred to as alerts.
Thresholds for warning and critical statuses are set on the monitor.
For example, from the disk storage metrics mentioned earlier, the percentage of free storage available can be calculated.
The warning threshold could be set at 30 percent and the critical threshold could be set at 20 percent.

In this chapter, we will learn about monitors and alerts and how they are implemented in Datadog, in detail.
Specifically, we will cover the following topics:
* Setting up monitors
* Managing monitors
* Distributing notifications
* Configuring downtime

### Setting up monitors

In a generic monitoring system, a monitor is usually tied to metrics or events, and Datadog covers these and much more.
There are multiple monitor types based on the data sources defined in Datadog.
Some of the most important ones include the following:
* **Metric**: As mentioned at the beginning of this chapter, metrics are the most common type of information used to build monitors.
A metric type monitor is based on the user-defined thresholds set for the metric value.
* Event: This monitors the system events tracked by Datadog.
* Host: This checks whether the Datadog agent on the host reports into the Datadog Software as a Service (SaaS) backend.
* Live process: This monitors whether a set of processes at the operating system level are running on one or a group of hosts.
* Process check: This monitors whether a process tracked by Datadog is running on one or a group of hosts.
* Network: These monitors check on the status of the TCP/HTTP endpoints.
* Custom check: These monitors are based on the custom checks run by the Datadog agent.

Now, we will look at how monitors of these types are created and what sort of instrumentation needs to be done to support them on the Datadog agent side.

By clicking on this option, you will get to an elaborate form where all the information needed for the monitor will be provided to create it.
As there are several options available when setting up a monitor, this form is rather long.
We will view it in the following screenshots:

The first step is to select the detection method.
In this example, as shown in *Figure 7.3*, the detection method is selected as **Threshold Alert**.
This is the usual detection method in which the metric value is compared with a static threshold value to trigger an alert.

The following list highlights other detection methods that are available:
* Change Alert: This compares the current metric value with a past value in the time series.
* Anomaly Detection: This detects any abnormality in the metric time series data based on past behavior.
* Outliers Alert: This alerts you about the unusual behavior of a member in a group. For example, increased usage of memory on a specific host in a cluster of hosts grouped by a tag.
*Forecast Alert: This is similar to a threshold alert; however, a forecasted metric value is compared with the static threshold.

The selection of the monitor type is important, as the behavior and use of a monitor largely depend on the type of monitor.

In the second step, the metric used for the monitor is selected, and filter conditions are added using tags to define the scope of the monitor.
Let's take a look at each field, in this section, to understand all of the available options:
* **Metric**: The metric used for the monitor is selected here.
It should be one of the metrics that is reported by the Datadog agent.
* **from**: Using available tags, the scope of the monitor is defined.
In this example, by selecting the **host** and **device_name** tags, the monitor is defined specifically for a disk partition on a host.
* **excluding**: Tags could be selected here to exclude more entities explicitly.

If the filter condition (set in the **from** and **excluding** fields) returns more than one set of metric values, such as **system.disk.free** values from multiple hosts and devices, one of the aggregate functions, **average**, **maximum**, **minimum**, or **sum**, will be selected to specify which value can be used for metric value comparisons.
Picking the correct aggregate function and tags for grouping is important.
If the filter condition returns only one value already, then this setting is irrelevant.

The alert triggered by this monitor could be configured as **Simple Alert** or **Multi Alert** (in *Figure 7.3*, *Simple Alert* is selected).
When the scope of the monitor is a single source, such as a storage device on a host, a simple alert is chosen.
If there are multiple sources involved and the Simple Alert option is chosen for the monitor, an aggregated alert notification is sent out for all the sources, and an alert notification that is specific to each source is sent out if the **Multi Alert** option is chosen.

Besides the **warning** and **alert** notifications, the monitor can also report on missing data.
If the **Notify** option is selected, a notification will be sent by the monitor when the metric values are not reported during the specified time window in minutes.
This option is chosen when the sources are expected to report metric values during normal operational conditions, and a **no data** alert indicates some issue in the application system monitored by Datadog.
If the sources selected in the monitor are not expected to report metric values regularly, choose the **Do not notify** option.
The no data alert could be set to resolve automatically after a specified time window.
Usually, this is set to **Never** unless there is a special case of it being automatically resolved.

Using the Delay evaluation option, the metric values used to evaluate various thresholds set in the monitor can be offset by a specified number of seconds.
For example, if a 60-second evaluation delay is specified, at 10:00, the checks for the monitor will be run on the data reported from 9:54 to 9:59 for a 5-minute data window.
In a public cloud environment, Datadog recommends as much as a 15-minute delay to reliably ensure that the metric values are available.

Based on the thresholds set for the monitor, the live status is charted at the top of the form, and the following screenshot shows how the sample monitor depicts the status it determined:

The light yellow area of the chart shows the range for the warning threshold, and the light purple area indicates the critical (alert) threshold.
The bold blue line above these areas tracks the current value of the metric, and of course, it is well above the danger zone.
While building a new monitor interactively, this chart is helpful to set realistic thresholds on the monitor.

In the fourth section of the **New Monitor** form, the notification to be sent out when a warning or critical (alert) state is triggered by the monitor.
It is presented with a template, as shown in the following screenshot:

The notification can have a title and body, just like an email message.
Template variables and conditionals could also be used to make them dynamic.
The following screenshot indicates what an actual notification template might look like:
